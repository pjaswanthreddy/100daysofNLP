{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text\n",
    "    - Bag of Words Model\n",
    "        - Representation that turns arbitary text into fixed-length vectors by frequencies \n",
    "        - Often called as Vectorization\n",
    "        - They lose the order of the words and grammar\n",
    "    - One Hot Encoding\n",
    "        - If word exists value has 1 else 0\n",
    "        - Doesn't attach any importance to the word\n",
    "    - TF/IDF Term Weighting\n",
    "        - Term Frequency * Inverse Term Ferquency\n",
    "        - If term appears frequently in a document, it's important give the term high score\n",
    "        - If a term apprears in many documents, it's not a unique identifier - give the term a low score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"the cat sat\",\n",
    "    \"the cat sat in the hat\",\n",
    "    \"the cat with the hat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'cat', 'sat', 'hat', 'the', 'with']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(set(' '.join(documents).split()))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'cat', 'sat', 'hat', 'the', 'with']\n",
      "the cat sat [0, 1, 1, 0, 1, 0]\n",
      "the cat sat in the hat [1, 1, 1, 1, 2, 0]\n",
      "the cat with the hat [0, 1, 0, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "bow_vectors = []\n",
    "for each_doc in documents:\n",
    "    doc_words = each_doc.split()\n",
    "    doc_vector = []\n",
    "    for x in vocabulary:\n",
    "        i = doc_words.count(x)\n",
    "        doc_vector.append(i)\n",
    "    bow_vectors.append(doc_vector)\n",
    "\n",
    "print(vocabulary)\n",
    "for i, x in enumerate(bow_vectors):\n",
    "    print(documents[i], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'cat', 'sat', 'hat', 'the', 'with']\n",
      "the cat sat [0, 1, 1, 0, 1, 0]\n",
      "the cat sat in the hat [1, 1, 1, 1, 1, 0]\n",
      "the cat with the hat [0, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "ohc_vectors = []\n",
    "for each_doc in documents:\n",
    "    words = each_doc.split()\n",
    "    doc_vector = []\n",
    "    for word in vocabulary:\n",
    "        i = words.count(word)\n",
    "        if i > 0:\n",
    "            i = 1\n",
    "        doc_vector.append(i)\n",
    "    ohc_vectors.append(doc_vector)\n",
    "print(vocabulary)\n",
    "for i, x in enumerate(ohc_vectors):\n",
    "    print(documents[i], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of documents = N\n",
    "Number of Documents with Term: Nt\n",
    "\n",
    "Normalized Term Frequency (TF):\n",
    "    - Number of times word appears in document / Total number of terms in the document\n",
    "    - More frequent term higher TF\n",
    "\n",
    "Inverse Document Frequency (IDF):\n",
    "    - IDF = log(N/ (1 + Nt))\n",
    "\n",
    "TF - IDF:\n",
    "    TF * IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = '''Most college students are not equipped with the skills necessary to project three-dimensional meaning onto a two-dimensional canvass of paper. And, unfortunately, they are not provided with more than a benevolent injunction: go and write. But how effective is this diktat? If you ever wriggled in front of a blank page, you know that its usefulness borders on zero. Even if you don’t struggle with writing, it does not necessarily mean you live up to your literary potential. Do your readers hold their breath when poring over your essay? No? Well, they could. If, of course, you learn how to attack them with a series of short, punchy sentences that are simple enough to get past their guard. Your readers would hold their breath until the last full stop if you also treat them to long, meaty sentences that have enough substance to nourish their hunger for quality. You got to learn how to do it. Here you can do that for free by learning from the best!\n",
    "'''\n",
    "document2 = '''A decent sample of essay writing could teach you that good content should be neither thunderously pretentious nor placidly banal. A mind shamelessly aggrandizing and frolicking on a page is rarely palatable; the same applies to muffled intelligence. It’s all about balance. A top-notch academic essay example could also inspire you to let your literary style shape the message. Your own voice should be clear, distinctive, and, above all, heard through the fuzz of text.\n",
    "'''\n",
    "document3 = '''It might happen that the topic is secondary to your needs while you have to get a better idea of how a particular type of paper is crafted. We've taken full care of that and systemized our essays writing examples according to their type. Thus, if you need, for instance, a great argumentative essay sample (be it on some legal, environmental or literary topic), getting to it is as simple as just clicking on the respective category in the table.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "documents_list = [document1, document2, document3]\n",
    "\n",
    "document1_words = document1.split()\n",
    "document2_words = document2.split()\n",
    "document3_words = document3.split()\n",
    "\n",
    "\n",
    "N = len(documents_list)\n",
    "overall_tfidf = []\n",
    "\n",
    "for document in documents_list:\n",
    "    doc_tf_idf = {}\n",
    "    document_words = document.split()\n",
    "    for x in document_words:\n",
    "        # calculate tf\n",
    "        tf = document_words.count(x) / len(document_words)\n",
    "        # calculate idf\n",
    "        nwdt = 0\n",
    "        for d in documents_list:\n",
    "            if x in d:\n",
    "                nwdt += 1\n",
    "        \n",
    "        idf = math.log(N/(1 + nwdt))\n",
    "\n",
    "        doc_tf_idf[x] = tf*idf\n",
    "\n",
    "    overall_tfidf.append(doc_tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('with', 0.009653931145432485), ('they', 0.004826965572716242), ('readers', 0.004826965572716242)]\n",
      "[('should', 0.010812402882884384), ('decent', 0.005406201441442192), ('teach', 0.005406201441442192)]\n",
      "[('might', 0.005068313851352055), ('happen', 0.005068313851352055), ('topic', 0.005068313851352055)]\n"
     ]
    }
   ],
   "source": [
    "for x in overall_tfidf:\n",
    "    y = sorted(x.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(sentence, n=2, r=2):\n",
    "    \"\"\"\n",
    "    Function which create ngrams and returns\n",
    "    top 5 n grams\n",
    "    \"\"\"\n",
    "\n",
    "    # Create n grams\n",
    "    ngrams = []\n",
    "    final_dict = {}\n",
    "    words = sentence.split()\n",
    "    if n < len(words):\n",
    "        for i in range(0, len(words)+1-n):\n",
    "            ng = ' '.join(words[i: i+n])\n",
    "            ngrams.append(ng)\n",
    "    \n",
    "    if ngrams:\n",
    "        uniq_ngrams = list(set(ngrams))\n",
    "        for x in uniq_ngrams:\n",
    "            final_dict[x] = ngrams.count(x)\n",
    "\n",
    "        sorted_dict = sorted(final_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_dict[:r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('testing module', 3), ('a testing', 2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ''' this is a testing module this is a testing module testing module'''\n",
    "create_ngrams(sentence, 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
