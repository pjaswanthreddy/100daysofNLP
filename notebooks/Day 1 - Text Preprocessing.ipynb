{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Text Processing Techniques\n",
    "    * Remove punctuations/Digits\n",
    "    * Remove short/stop words\n",
    "    * Stemming\n",
    "    * Lemmatization\n",
    "    * Word Tokenize\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''Lorem ipsum dolor sit amet, ad case natum duo, in habeo novum consequuntur quo, erant option vim ad.\n",
    "    Eu fugit voluptua antiopam ius, mel graeco patrioque scripserit ad, vis id justo graeco.\n",
    "    Dicam munere nemore cum no, mei id erat commodo postulant, eam at dicta iisque scripserit.\n",
    "    No ius zril solet veniam, ei sea labores eleifend inciderint, eu convenire evertitur incorrupte pro.\n",
    "    2003 was the greatest Year in the history since 1900\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum dolor sit amet ad case natum duo in habeo novum consequuntur quo erant option vim ad\\n    Eu fugit voluptua antiopam ius mel graeco patrioque scripserit ad vis id justo graeco\\n    Dicam munere nemore cum no mei id erat commodo postulant eam at dicta iisque scripserit\\n    No ius zril solet veniam ei sea labores eleifend inciderint eu convenire evertitur incorrupte pro\\n    2003 was the greatest Year in the history since 1900\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "clean_text = regex.sub('', sample_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum dolor sit amet ad case natum duo in habeo novum consequuntur quo erant option vim ad\\n    Eu fugit voluptua antiopam ius mel graeco patrioque scripserit ad vis id justo graeco\\n    Dicam munere nemore cum no mei id erat commodo postulant eam at dicta iisque scripserit\\n    No ius zril solet veniam ei sea labores eleifend inciderint eu convenire evertitur incorrupte pro\\n     was the greatest Year in the history since \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers\n",
    "regex = re.compile('[%s]' % re.escape(string.digits))\n",
    "clean_text = regex.sub('', clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum dolor amet case natum habeo novum consequuntur erant option fugit voluptua antiopam graeco patrioque scripserit justo graeco Dicam munere nemore erat commodo postulant dicta iisque scripserit zril solet veniam labores eleifend inciderint convenire evertitur incorrupte greatest Year history since'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove short words\n",
    "clean_text = ' '.join([ x for x in clean_text.split() if len(x) >=4])\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'authentic pieces art'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import stop words\n",
    "sample_text1 = '''\n",
    "    this has to be the most authentic pieces of art\n",
    "'''\n",
    "sw = stopwords.words(\"english\")\n",
    "cleaned_text1 = ' '.join([x for x in sample_text1.split() if x not in sw])\n",
    "cleaned_text1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "    * Stemming is process of reducing inflection in words to their root forms even if the stem in itself is not a valid word.\n",
    "    * Two stemmers in NLTK package\n",
    "        * Porter Stemmer\n",
    "            - Simplicity and speed\n",
    "            - Does not follow linguistics\n",
    "            - Does not check if the final stem is a valid word/not\n",
    "        * Lancaster Stemmer\n",
    "            - Simple but heavy due to over iterations\n",
    "            - On each iteration, tries to find applicable rule by last character of the word\n",
    "            - Overstemming causes the stems to not be linguistic/ they won't have any meaning\n",
    "            - Rules are saved externally, one table containing 120 rules indexed by last letter of suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                ** Porter Stemmer   ** Lancaster Stemmer\n",
      "cats                cat                 cat                 \n",
      "acheive             acheiv              acheiv              \n",
      "acheiving           acheiv              acheiv              \n",
      "acheived            acheiv              acheiv              \n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "words = [\"cats\", \"acheive\", \"acheiving\", \"acheived\"]\n",
    "\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\", \"** Porter Stemmer\", \"** Lancaster Stemmer\"))\n",
    "for x in words:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(x, porter.stem(x), lancaster.stem(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "* Lemmatization reduces the inflected word to their root word of the language\n",
    "* WordNetLemmatizer\n",
    "    - You need to provide the context if you want to lemmatize parts-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                ** WordNet Lemma    \n",
      "cats                cat                 \n",
      "eating              eat                 \n",
      "ate                 eat                 \n",
      "eats                eat                 \n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "words = [\"cats\", \"eating\", \"ate\", \"eats\"]\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\", \"** WordNet Lemma\"))\n",
    "for x in words:\n",
    "    print(\"{0:20}{1:20}\".format(x, lemma.lemmatize(x, pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "    - Whenever tokenizing always decode the string\n",
    "    - word_tokenize - Divides strings into list of words\n",
    "    - wordpunct_tokenize - Divides strings based on puncuation, text and whitespace\n",
    "    - sent_tokenize - If the text contains multiple sentences, and you want to operate on level of sentences\n",
    "    - Multiple ways to tokenize text, WhiteSpaceTokenizer represents a word in a sentence as tuple. This would be\n",
    "      easier for comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Punct Tokenize: ['A', 'Chelsea', 'tee', 'shirt', 'costs', '$', '100', '.', 'Could', 'you', 'buy', 'me', '3', 'of', 'them', '?', 'Thank', 'you', '!']\n",
      "Sent Tokenize: ['A Chelsea tee shirt costs $100.', 'Could you buy me 3 of them?', 'Thank you!']\n",
      "Sent-Word Tokenize: [['A', 'Chelsea', 'tee', 'shirt', 'costs', '$', '100', '.'], ['Could', 'you', 'buy', 'me', '3', 'of', 'them', '?'], ['Thank', 'you', '!']]\n"
     ]
    }
   ],
   "source": [
    "s = '''A Chelsea tee shirt costs $100. Could you buy me 3 of them? Thank you!'''\n",
    "print(\"Word Punct Tokenize:\", wordpunct_tokenize(s))\n",
    "print(\"Sent Tokenize:\", sent_tokenize(s))\n",
    "print(\"Sent-Word Tokenize:\", [word_tokenize(x) for x in sent_tokenize(s)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (2, 9),\n",
       " (10, 13),\n",
       " (14, 19),\n",
       " (20, 25),\n",
       " (26, 31),\n",
       " (32, 37),\n",
       " (38, 41),\n",
       " (42, 45),\n",
       " (46, 48),\n",
       " (49, 50),\n",
       " (51, 53),\n",
       " (54, 59),\n",
       " (60, 65),\n",
       " (66, 70)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(WhitespaceTokenizer().span_tokenize(s))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43534900004dcca7746baaa7d615ad420cbebe215fcd9c5ec49198e1f7477562"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
